{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9f9b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5da03cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=r\"C:\\Users\\Rohit Varre\\Desktop\\MliS\\Python\\SYN data\"\n",
    "categories=os.listdir(data_path)\n",
    "labels = [i for i in range(len(categories))]\n",
    "code=dict(zip(labels,categories))     # Dictionary to extract categories from labels\n",
    "cap = cv2.VideoCapture(0)             # Opencv command to access the webcamera\n",
    "flag_start_capturing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37c6814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w'}\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8354e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 120)               7320      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 23)                713       \n",
      "=================================================================\n",
      "Total params: 17,123\n",
      "Trainable params: 17,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = load_model('mediapipe.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03c456b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe methods to extract and display hand cooridnates\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.4,\n",
    "    max_num_hands=1,\n",
    "    min_tracking_confidence=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "68b6637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops over the frames during the video\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    image = cv2.flip(frame,1)\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = True\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            for i in hand_landmarks.landmark:\n",
    "                x.append(i.x)\n",
    "                y.append(i.y)\n",
    "                z.append(i.z)\n",
    "\n",
    "    keypress = cv2.waitKey(5)\n",
    "    if keypress == 32:                                # Click spacebar to start or stop predictions\n",
    "        if flag_start_capturing == False:\n",
    "            flag_start_capturing = True\n",
    "        else:\n",
    "            flag_start_capturing = False\n",
    "    if flag_start_capturing == True and results.multi_hand_landmarks:\n",
    "        df = pd.DataFrame({'x':x,'y':y,'z':z})\n",
    "        data = np.array(df)\n",
    "        data = data-data[0]                           # Same data scaling done during training\n",
    "        data = data[1:]\n",
    "        data = np.reshape(data,(-1,60))\n",
    "        out = model.predict(data)\n",
    "        output=np.argmax(out,axis=1)[0]\n",
    "        cv2.putText(image, code[output], (30,400), cv2.FONT_HERSHEY_TRIPLEX, 2.5, (127,127,255))\n",
    "        \n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    \n",
    "    if keypress == 27:                                # Click escape to end the program\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e618e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
